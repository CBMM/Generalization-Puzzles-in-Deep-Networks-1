/home/slurm/slurmd/job9699847/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  51
SLURM_JOBID =  9699847

--Degree_mdl=8 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=51 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.4252848]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841  6.1477  5.4685  4.7929
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841 -6.1477 -5.4685 -4.7929
[torch.FloatTensor of size 1x9]

-------------
i = 80000, current_train_loss = [ 0.32340485]
W.data = 

Columns 0 to 7 
 -0.3121   3.3376  -4.7478  -8.4310   3.4944  11.5015   9.9195   0.1324

Columns 8 to 8 
-14.8332
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-03 *
  0.0070 -0.0458 -0.3429  1.0537  0.1610 -0.7866 -0.9075 -0.2020  1.0705
[torch.FloatTensor of size 1x9]

-------------
i = 160000, current_train_loss = [ 0.29661259]
W.data = 

Columns 0 to 7 
 -0.3278   2.8695  -0.0601 -16.5733   1.1433  16.4388  16.3717   2.0067

Columns 8 to 8 
-21.8170
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
  0.0007  1.1209 -7.0536  9.8803  3.5758 -5.2390 -7.4611 -2.4769  7.6423
[torch.FloatTensor of size 1x9]

-------------
i = 240000, current_train_loss = [ 0.27302152]
W.data = 

Columns 0 to 7 
 -0.3184   1.8869   5.7188 -24.3071  -1.7996  20.3756  22.1166   3.9788

Columns 8 to 8 
-27.6033
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0156  1.2773 -7.2435  9.4406  3.6993 -4.6907 -6.9480 -2.4211  6.9576
[torch.FloatTensor of size 1x9]

-------------
i = 320000, current_train_loss = [ 0.25152335]
W.data = 

Columns 0 to 7 
 -0.3061   0.8749  11.4162 -31.6872  -4.6999  24.0291  27.5320   5.8821

Columns 8 to 8 
-32.9954
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0073  1.2534 -6.9928  9.0243  3.5640 -4.4315 -6.5751 -2.2569  6.6725
[torch.FloatTensor of size 1x9]

-------------
i = 400000, current_train_loss = [ 0.2318639]
W.data = 

Columns 0 to 7 
 -0.2941  -0.0956  16.8698 -38.7364  -7.4890  27.5152  32.7232   7.7075

Columns 8 to 8 
-38.1571
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
  0.0194  1.2254 -6.7354  8.6142  3.3950 -4.2424 -6.2889 -2.1773  6.3184
[torch.FloatTensor of size 1x9]

-------------
i = 480000, current_train_loss = [ 0.21395904]
W.data = 

Columns 0 to 7 
 -0.2822  -1.0305  22.0962 -45.4789 -10.1401  30.8621  37.6187   9.4431

Columns 8 to 8 
-43.0470
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0028  1.1535 -6.3930  8.2715  3.2295 -4.0965 -6.0616 -2.1456  5.9450
[torch.FloatTensor of size 1x9]

-------------
i = 560000, current_train_loss = [ 0.19760652]
W.data = 

Columns 0 to 7 
 -0.2709  -1.9263  27.1032 -51.9418 -12.6492  34.0351  42.3040  11.0890

Columns 8 to 8 
-47.7037
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0316  1.0748 -6.1722  7.8193  2.9755 -4.0038 -5.8361 -2.0536  5.6990
[torch.FloatTensor of size 1x9]

-------------
i = 640000, current_train_loss = [ 0.18263234]
W.data = 

Columns 0 to 7 
 -0.2604  -2.7759  31.8752 -58.1231 -15.0526  37.0875  46.8040  12.6694

Columns 8 to 8 
-52.1867
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0148  1.0652 -5.8853  7.5358  2.9129 -3.7413 -5.4752 -1.8591  5.5283
[torch.FloatTensor of size 1x9]

-------------
i = 720000, current_train_loss = [ 0.16895609]
W.data = 

Columns 0 to 7 
 -0.2499  -3.5949  36.4561 -64.0498 -17.3441  40.0263  51.0764  14.1588

Columns 8 to 8 
-56.4436
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0522  0.9661 -5.5939  7.2278  2.7506 -3.6431 -5.3172 -1.8825  5.1416
[torch.FloatTensor of size 1x9]

--- 608.3200659751892 seconds ---
--- 10.138667766253153 minutes ---
--- 0.16897779610421923 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
