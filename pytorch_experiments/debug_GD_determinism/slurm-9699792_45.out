/home/slurm/slurmd/job9699840/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  45
SLURM_JOBID =  9699840

--Degree_mdl=7 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=45 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.42543319]
W.data = 

Columns 0 to 5 
-2.7992e-17  8.2479e-03  8.2479e-03  7.7896e-03  7.3314e-03  6.7841e-03

Columns 6 to 7 
 6.1477e-03  5.4685e-03
[torch.FloatTensor of size 1x8]

W.grad.data = 

Columns 0 to 5 
 2.7992e-16 -8.2479e-02 -8.2479e-02 -7.7896e-02 -7.3314e-02 -6.7841e-02

Columns 6 to 7 
-6.1477e-02 -5.4685e-02
[torch.FloatTensor of size 1x8]

-------------
i = 80000, current_train_loss = [ 0.35714471]
W.data = 
 -0.2700   2.8360  -5.4458  -5.2142   6.4380  10.7190   3.5493 -12.5228
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-03 *
  0.0084 -0.1382  0.0744  0.7871 -0.3201 -0.9669 -0.4650  1.0159
[torch.FloatTensor of size 1x8]

-------------
i = 160000, current_train_loss = [ 0.33891544]
W.data = 
 -0.3033   3.1418  -3.8770 -11.7177   7.5047  17.1710   7.1858 -19.0293
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0110  0.2516 -3.6920  8.2520 -0.1519 -7.0353 -4.4940  6.7713
[torch.FloatTensor of size 1x8]

-------------
i = 240000, current_train_loss = [ 0.32382479]
W.data = 
 -0.3071   2.7279  -0.3855 -18.3184   7.2264  22.4081  10.7123 -23.9924
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0039  0.6627 -4.7746  8.2315  0.6665 -6.2025 -4.3438  5.8075
[torch.FloatTensor of size 1x8]

-------------
i = 320000, current_train_loss = [ 0.30958447]
W.data = 
 -0.3031   2.1333   3.5492 -24.8398   6.6170  27.2239  14.1617 -28.4735
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0077  0.7703 -4.9571  8.0972  0.8643 -5.8920 -4.2481  5.4316
[torch.FloatTensor of size 1x8]

-------------
i = 400000, current_train_loss = [ 0.29600614]
W.data = 
 -0.2972   1.5029   7.5289 -31.2268   5.9256  31.8657  17.5191 -32.7515
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0094  0.7766 -4.9266  7.9111  0.8671 -5.7622 -4.2180  5.1551
[torch.FloatTensor of size 1x8]

-------------
i = 480000, current_train_loss = [ 0.28301769]
W.data = 
 -0.2911   0.8747  11.4568 -37.4775   5.2056  36.4121  20.7996 -36.9152
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0225  0.7811 -4.8575  7.7441  0.9084 -5.5379 -4.0171  5.1334
[torch.FloatTensor of size 1x8]

-------------
i = 560000, current_train_loss = [ 0.27073818]
W.data = 
 -0.2844   0.2450  15.3280 -43.5769   4.5468  40.7085  23.9976 -40.9009
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0077  0.7893 -4.7742  7.4557  0.7699 -5.4717 -3.9023  5.1224
[torch.FloatTensor of size 1x8]

-------------
i = 640000, current_train_loss = [ 0.25897881]
W.data = 
 -0.2783  -0.3511  19.0494 -49.5182   3.9115  44.9809  27.1357 -44.8681
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0094  0.7266 -4.5672  7.4263  0.8425 -5.3441 -3.9137  4.7939
[torch.FloatTensor of size 1x8]

-------------
i = 720000, current_train_loss = [ 0.24768929]
W.data = 
 -0.2727  -0.9364  22.7073 -55.3262   3.2053  49.2414  30.1943 -48.7529
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0044  0.7474 -4.5202  7.2459  0.8602 -5.1475 -3.7296  4.7792
[torch.FloatTensor of size 1x8]

--- 644.1690580844879 seconds ---
--- 10.7361509680748 minutes ---
--- 0.17893584946791333 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
