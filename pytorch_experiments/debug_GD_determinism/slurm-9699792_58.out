/home/slurm/slurmd/job9699854/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  58
SLURM_JOBID =  9699854

--Degree_mdl=8 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=58 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.4252848]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841  6.1477  5.4685  4.7929
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841 -6.1477 -5.4685 -4.7929
[torch.FloatTensor of size 1x9]

-------------
i = 80000, current_train_loss = [ 0.32340473]
W.data = 

Columns 0 to 7 
 -0.3121   3.3376  -4.7478  -8.4310   3.4945  11.5015   9.9194   0.1324

Columns 8 to 8 
-14.8333
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-03 *
  0.0070 -0.0458 -0.3429  1.0537  0.1610 -0.7867 -0.9076 -0.2021  1.0704
[torch.FloatTensor of size 1x9]

-------------
i = 160000, current_train_loss = [ 0.29661223]
W.data = 

Columns 0 to 7 
 -0.3278   2.8695  -0.0601 -16.5735   1.1433  16.4388  16.3718   2.0067

Columns 8 to 8 
-21.8171
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0062  1.1147 -7.0595  9.8745  3.5703 -5.2444 -7.4664 -2.4822  7.6370
[torch.FloatTensor of size 1x9]

-------------
i = 240000, current_train_loss = [ 0.27302122]
W.data = 

Columns 0 to 7 
 -0.3184   1.8869   5.7188 -24.3073  -1.7996  20.3755  22.1168   3.9789

Columns 8 to 8 
-27.6035
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0285  1.2669 -7.2541  9.4299  3.6885 -4.7015 -6.9589 -2.4320  6.9467
[torch.FloatTensor of size 1x9]

-------------
i = 320000, current_train_loss = [ 0.25152254]
W.data = 

Columns 0 to 7 
 -0.3061   0.8749  11.4164 -31.6874  -4.6999  24.0293  27.5320   5.8822

Columns 8 to 8 
-32.9956
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0190  1.2453 -7.0006  9.0156  3.5542 -4.4424 -6.5867 -2.2693  6.6594
[torch.FloatTensor of size 1x9]

-------------
i = 400000, current_train_loss = [ 0.23186259]
W.data = 

Columns 0 to 7 
 -0.2941  -0.0956  16.8699 -38.7369  -7.4888  27.5151  32.7236   7.7079

Columns 8 to 8 
-38.1578
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0148  1.1994 -6.7582  8.5935  3.3754 -4.2616 -6.3080 -2.1966  6.2990
[torch.FloatTensor of size 1x9]

-------------
i = 480000, current_train_loss = [ 0.21395749]
W.data = 

Columns 0 to 7 
 -0.2822  -1.0308  22.0971 -45.4795 -10.1402  30.8610  37.6193   9.4442

Columns 8 to 8 
-43.0476
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0116  1.1463 -6.3982  8.2650  3.2224 -4.1036 -6.0683 -2.1517  5.9397
[torch.FloatTensor of size 1x9]

-------------
i = 560000, current_train_loss = [ 0.19760546]
W.data = 

Columns 0 to 7 
 -0.2709  -1.9263  27.1034 -51.9425 -12.6487  34.0352  42.3035  11.0903

Columns 8 to 8 
-47.7046
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0227  1.0793 -6.1694  7.8218  2.9782 -4.0012 -5.8336 -2.0513  5.7010
[torch.FloatTensor of size 1x9]

-------------
i = 640000, current_train_loss = [ 0.18263179]
W.data = 

Columns 0 to 7 
 -0.2604  -2.7758  31.8752 -58.1237 -15.0520  37.0873  46.8035  12.6708

Columns 8 to 8 
-52.1877
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0328  1.0468 -5.9037  7.5167  2.8926 -3.7630 -5.4983 -1.8835  5.5026
[torch.FloatTensor of size 1x9]

-------------
i = 720000, current_train_loss = [ 0.16895539]
W.data = 

Columns 0 to 7 
 -0.2499  -3.5948  36.4561 -64.0503 -17.3437  40.0266  51.0759  14.1599

Columns 8 to 8 
-56.4444
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0116  0.9988 -5.5620  7.2600  2.7825 -3.6118 -5.2869 -1.8532  5.1699
[torch.FloatTensor of size 1x9]

--- 397.9604182243347 seconds ---
--- 6.632673637072245 minutes ---
--- 0.11054456061787075 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
