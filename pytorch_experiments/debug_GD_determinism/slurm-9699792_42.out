/home/slurm/slurmd/job9699837/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  42
SLURM_JOBID =  9699837

--Degree_mdl=7 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=42 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.42543319]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841  6.1477  5.4685
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841 -6.1477 -5.4685
[torch.FloatTensor of size 1x8]

-------------
i = 80000, current_train_loss = [ 0.35714442]
W.data = 
 -0.2700   2.8360  -5.4459  -5.2142   6.4381  10.7192   3.5493 -12.5230
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-03 *
  0.0087 -0.1377  0.0749  0.7875 -0.3197 -0.9665 -0.4647  1.0162
[torch.FloatTensor of size 1x8]

-------------
i = 160000, current_train_loss = [ 0.33891478]
W.data = 
 -0.3033   3.1419  -3.8771 -11.7177   7.5047  17.1713   7.1860 -19.0296
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0086  0.2493 -3.6946  8.2498 -0.1539 -7.0375 -4.4964  6.7686
[torch.FloatTensor of size 1x8]

-------------
i = 240000, current_train_loss = [ 0.32382452]
W.data = 
 -0.3071   2.7279  -0.3858 -18.3182   7.2262  22.4083  10.7127 -23.9929
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0124  0.6555 -4.7785  8.2288  0.6639 -6.2057 -4.3478  5.8026
[torch.FloatTensor of size 1x8]

-------------
i = 320000, current_train_loss = [ 0.30958384]
W.data = 
 -0.3031   2.1334   3.5491 -24.8394   6.6158  27.2251  14.1616 -28.4738
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0077  0.7684 -4.9610  8.0948  0.8640 -5.8905 -4.2448  5.4363
[torch.FloatTensor of size 1x8]

-------------
i = 400000, current_train_loss = [ 0.29600537]
W.data = 
 -0.2972   1.5029   7.5291 -31.2271   5.9249  31.8666  17.5193 -32.7519
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0007  0.7829 -4.9249  7.9127  0.8698 -5.7581 -4.2127  5.1615
[torch.FloatTensor of size 1x8]

-------------
i = 480000, current_train_loss = [ 0.28301576]
W.data = 
 -0.2911   0.8748  11.4570 -37.4781   5.2049  36.4140  20.7994 -36.9159
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0004  0.7979 -4.8436  7.7581  0.9227 -5.5234 -4.0027  5.1475
[torch.FloatTensor of size 1x8]

-------------
i = 560000, current_train_loss = [ 0.27073449]
W.data = 
 -0.2844   0.2451  15.3280 -43.5775   4.5452  40.7123  23.9971 -40.9022
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0138  0.7666 -4.7946  7.4392  0.7558 -5.4849 -3.9155  5.1087
[torch.FloatTensor of size 1x8]

-------------
i = 640000, current_train_loss = [ 0.25897428]
W.data = 
 -0.2783  -0.3513  19.0506 -49.5195   3.9094  44.9848  27.1356 -44.8695
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0373  0.7019 -4.5898  7.4049  0.8217 -5.3644 -3.9337  4.7739
[torch.FloatTensor of size 1x8]

-------------
i = 720000, current_train_loss = [ 0.24768487]
W.data = 
 -0.2727  -0.9365  22.7083 -55.3274   3.2027  49.2461  30.1940 -48.7543
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0061  0.7452 -4.5230  7.2431  0.8572 -5.1511 -3.7338  4.7743
[torch.FloatTensor of size 1x8]

--- 412.28613567352295 seconds ---
--- 6.871435594558716 minutes ---
--- 0.1145239265759786 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
