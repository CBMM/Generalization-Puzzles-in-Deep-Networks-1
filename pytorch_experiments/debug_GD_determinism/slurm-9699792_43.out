/home/slurm/slurmd/job9699838/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  43
SLURM_JOBID =  9699838

--Degree_mdl=7 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=43 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.42543319]
W.data = 

Columns 0 to 5 
-4.1988e-17  8.2479e-03  8.2479e-03  7.7896e-03  7.3314e-03  6.7841e-03

Columns 6 to 7 
 6.1477e-03  5.4685e-03
[torch.FloatTensor of size 1x8]

W.grad.data = 

Columns 0 to 5 
 4.1988e-16 -8.2479e-02 -8.2479e-02 -7.7896e-02 -7.3314e-02 -6.7841e-02

Columns 6 to 7 
-6.1477e-02 -5.4685e-02
[torch.FloatTensor of size 1x8]

-------------
i = 80000, current_train_loss = [ 0.35714442]
W.data = 
 -0.2700   2.8360  -5.4459  -5.2142   6.4381  10.7192   3.5493 -12.5230
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-03 *
  0.0082 -0.1381  0.0745  0.7872 -0.3200 -0.9667 -0.4649  1.0161
[torch.FloatTensor of size 1x8]

-------------
i = 160000, current_train_loss = [ 0.33891484]
W.data = 
 -0.3033   3.1419  -3.8771 -11.7177   7.5046  17.1713   7.1860 -19.0297
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0048  0.2462 -3.6968  8.2484 -0.1549 -7.0381 -4.4968  6.7683
[torch.FloatTensor of size 1x8]

-------------
i = 240000, current_train_loss = [ 0.32382435]
W.data = 
 -0.3071   2.7279  -0.3857 -18.3183   7.2262  22.4084  10.7128 -23.9930
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0062  0.6740 -4.7608  8.2457  0.6798 -6.1905 -4.3333  5.8165
[torch.FloatTensor of size 1x8]

-------------
i = 320000, current_train_loss = [ 0.30958384]
W.data = 
 -0.3031   2.1334   3.5491 -24.8395   6.6157  27.2252  14.1617 -28.4739
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0098  0.7675 -4.9615  8.0947  0.8642 -5.8901 -4.2444  5.4367
[torch.FloatTensor of size 1x8]

-------------
i = 400000, current_train_loss = [ 0.29600531]
W.data = 
 -0.2972   1.5029   7.5290 -31.2269   5.9246  31.8669  17.5194 -32.7521
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0128  0.7717 -4.9334  7.9052  0.8625 -5.7656 -4.2205  5.1532
[torch.FloatTensor of size 1x8]

-------------
i = 480000, current_train_loss = [ 0.28301623]
W.data = 
 -0.2911   0.8748  11.4569 -37.4778   5.2046  36.4141  20.7994 -36.9158
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0225  0.7798 -4.8606  7.7419  0.9072 -5.5383 -4.0170  5.1338
[torch.FloatTensor of size 1x8]

-------------
i = 560000, current_train_loss = [ 0.27073476]
W.data = 
 -0.2844   0.2452  15.3279 -43.5772   4.5447  40.7127  23.9971 -40.9023
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0155  0.7685 -4.7928  7.4410  0.7576 -5.4833 -3.9141  5.1100
[torch.FloatTensor of size 1x8]

-------------
i = 640000, current_train_loss = [ 0.25897408]
W.data = 
 -0.2783  -0.3513  19.0505 -49.5193   3.9089  44.9851  27.1357 -44.8696
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0106  0.7270 -4.5654  7.4289  0.8456 -5.3408 -3.9103  4.7970
[torch.FloatTensor of size 1x8]

-------------
i = 720000, current_train_loss = [ 0.24768521]
W.data = 
 -0.2727  -0.9365  22.7082 -55.3271   3.2022  49.2464  30.1940 -48.7543
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0043  0.7357 -4.5306  7.2368  0.8519 -5.1554 -3.7375  4.7713
[torch.FloatTensor of size 1x8]

--- 392.50340366363525 seconds ---
--- 6.541723394393921 minutes ---
--- 0.10902872323989869 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
