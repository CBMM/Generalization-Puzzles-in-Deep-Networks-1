/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  28
SLURM_JOBID =  9699823

--Degree_mdl=5 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=28 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.42593381]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841
[torch.FloatTensor of size 1x6]

-------------
i = 80000, current_train_loss = [ 0.40822023]
W.data = 
-0.1911  0.8607 -1.9736  0.2909  4.6952 -3.5193
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0218 -0.6992  2.1972  0.2819 -5.4701  3.7025
[torch.FloatTensor of size 1x6]

-------------
i = 160000, current_train_loss = [ 0.40492564]
W.data = 
-0.2058  1.3203 -3.3413 -0.2360  8.7952 -6.1767
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0182 -0.4603  1.2784  0.9942 -4.8138  2.9839
[torch.FloatTensor of size 1x6]

-------------
i = 240000, current_train_loss = [ 0.4024545]
W.data = 
 -0.2161   1.6205  -4.0875  -1.2492  12.4382  -8.3555
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0127 -0.2920  0.6227  1.5026 -4.3371  2.4831
[torch.FloatTensor of size 1x6]

-------------
i = 320000, current_train_loss = [ 0.40040395]
W.data = 
 -0.2234   1.8070  -4.3909  -2.6083  15.7557 -10.1935
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0119 -0.1723  0.1696  1.8826 -3.9737  2.1487
[torch.FloatTensor of size 1x6]

-------------
i = 400000, current_train_loss = [ 0.39856583]
W.data = 
 -0.2284   1.9131  -4.3787  -4.2179  18.8467 -11.7909
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0056 -0.0845 -0.1710  2.1346 -3.7232  1.9065
[torch.FloatTensor of size 1x6]

-------------
i = 480000, current_train_loss = [ 0.39684466]
W.data = 
 -0.2318   1.9609  -4.1413  -5.9986  21.7630 -13.2103
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0022 -0.0286 -0.4080  2.3211 -3.5316  1.7480
[torch.FloatTensor of size 1x6]

-------------
i = 560000, current_train_loss = [ 0.39518979]
W.data = 
 -0.2341   1.9668  -3.7445  -7.8951  24.5483 -14.5012
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0034  0.0159 -0.5643  2.4392 -3.4323  1.5739
[torch.FloatTensor of size 1x6]

-------------
i = 640000, current_train_loss = [ 0.39354855]
W.data = 
 -0.2356   1.9470  -3.2416  -9.8918  27.2867 -15.7256
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
 -0.0012  0.0371 -0.6940  2.5274 -3.3392  1.4743
[torch.FloatTensor of size 1x6]

-------------
i = 720000, current_train_loss = [ 0.39195433]
W.data = 
 -0.2365   1.9005  -2.6410 -11.9390  29.9013 -16.8472
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0014  0.0646 -0.7787  2.5778 -3.2734  1.4257
[torch.FloatTensor of size 1x6]

--- 647.450870513916 seconds ---
--- 10.7908478418986 minutes ---
--- 0.17984746403164334 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
