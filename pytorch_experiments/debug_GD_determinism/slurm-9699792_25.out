/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  25
SLURM_JOBID =  9699820

--Degree_mdl=5 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=25 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.42593381]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841
[torch.FloatTensor of size 1x6]

-------------
i = 80000, current_train_loss = [ 0.40822023]
W.data = 
-0.1911  0.8607 -1.9736  0.2909  4.6952 -3.5192
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0215 -0.6997  2.1968  0.2816 -5.4703  3.7022
[torch.FloatTensor of size 1x6]

-------------
i = 160000, current_train_loss = [ 0.4049257]
W.data = 
-0.2058  1.3203 -3.3413 -0.2360  8.7952 -6.1767
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0125 -0.4652  1.2738  0.9900 -4.8180  2.9798
[torch.FloatTensor of size 1x6]

-------------
i = 240000, current_train_loss = [ 0.40245456]
W.data = 
 -0.2161   1.6205  -4.0874  -1.2492  12.4382  -8.3555
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0112 -0.2917  0.6230  1.5028 -4.3372  2.4827
[torch.FloatTensor of size 1x6]

-------------
i = 320000, current_train_loss = [ 0.40040389]
W.data = 
 -0.2234   1.8070  -4.3908  -2.6084  15.7557 -10.1935
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0066 -0.1775  0.1649  1.8782 -3.9778  2.1448
[torch.FloatTensor of size 1x6]

-------------
i = 400000, current_train_loss = [ 0.39856574]
W.data = 
 -0.2284   1.9131  -4.3787  -4.2179  18.8467 -11.7909
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
 -0.0008 -0.0892 -0.1749  2.1311 -3.7268  1.9031
[torch.FloatTensor of size 1x6]

-------------
i = 480000, current_train_loss = [ 0.3968448]
W.data = 
 -0.2318   1.9609  -4.1415  -5.9984  21.7631 -13.2104
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0015 -0.0300 -0.4095  2.3193 -3.5338  1.7453
[torch.FloatTensor of size 1x6]

-------------
i = 560000, current_train_loss = [ 0.39518982]
W.data = 
 -0.2341   1.9669  -3.7447  -7.8949  24.5483 -14.5012
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0000  0.0122 -0.5682  2.4356 -3.4356  1.5709
[torch.FloatTensor of size 1x6]

-------------
i = 640000, current_train_loss = [ 0.3935487]
W.data = 
 -0.2356   1.9470  -3.2418  -9.8914  27.2864 -15.7255
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
 -0.0013  0.0352 -0.6960  2.5258 -3.3404  1.4734
[torch.FloatTensor of size 1x6]

-------------
i = 720000, current_train_loss = [ 0.39195445]
W.data = 
 -0.2365   1.9005  -2.6409 -11.9388  29.9007 -16.8468
[torch.FloatTensor of size 1x6]

W.grad.data = 
1.00000e-04 *
  0.0024  0.0668 -0.7765  2.5797 -3.2714  1.4280
[torch.FloatTensor of size 1x6]

--- 454.48916888237 seconds ---
--- 7.574819481372833 minutes ---
--- 0.1262469913562139 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
