/home/slurm/slurmd/job9699850/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  54
SLURM_JOBID =  9699850

--Degree_mdl=8 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=54 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.4252848]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841  6.1477  5.4685  4.7929
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841 -6.1477 -5.4685 -4.7929
[torch.FloatTensor of size 1x9]

-------------
i = 80000, current_train_loss = [ 0.3234047]
W.data = 

Columns 0 to 7 
 -0.3121   3.3376  -4.7478  -8.4310   3.4945  11.5015   9.9194   0.1324

Columns 8 to 8 
-14.8333
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-03 *
  0.0068 -0.0460 -0.3431  1.0534  0.1608 -0.7869 -0.9079 -0.2024  1.0701
[torch.FloatTensor of size 1x9]

-------------
i = 160000, current_train_loss = [ 0.29661241]
W.data = 

Columns 0 to 7 
 -0.3278   2.8695  -0.0601 -16.5734   1.1433  16.4388  16.3718   2.0067

Columns 8 to 8 
-21.8171
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0089  1.1098 -7.0647  9.8691  3.5649 -5.2499 -7.4719 -2.4877  7.6316
[torch.FloatTensor of size 1x9]

-------------
i = 240000, current_train_loss = [ 0.27302098]
W.data = 

Columns 0 to 7 
 -0.3184   1.8869   5.7188 -24.3073  -1.7996  20.3756  22.1168   3.9790

Columns 8 to 8 
-27.6035
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0118  1.2814 -7.2400  9.4436  3.7017 -4.6886 -6.9462 -2.4196  6.9588
[torch.FloatTensor of size 1x9]

-------------
i = 320000, current_train_loss = [ 0.25152242]
W.data = 

Columns 0 to 7 
 -0.3061   0.8749  11.4164 -31.6874  -4.7000  24.0292  27.5321   5.8823

Columns 8 to 8 
-32.9957
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0244  1.2387 -7.0057  9.0116  3.5510 -4.4450 -6.5888 -2.2710  6.6580
[torch.FloatTensor of size 1x9]

-------------
i = 400000, current_train_loss = [ 0.23186269]
W.data = 

Columns 0 to 7 
 -0.2941  -0.0956  16.8699 -38.7369  -7.4889  27.5151  32.7236   7.7080

Columns 8 to 8 
-38.1578
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
  0.0124  1.2219 -6.7400  8.6086  3.3882 -4.2503 -6.2980 -2.1875  6.3073
[torch.FloatTensor of size 1x9]

-------------
i = 480000, current_train_loss = [ 0.21395704]
W.data = 

Columns 0 to 7 
 -0.2822  -1.0308  22.0972 -45.4795 -10.1403  30.8610  37.6195   9.4443

Columns 8 to 8 
-43.0478
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0133  1.1456 -6.3987  8.2647  3.2221 -4.1038 -6.0684 -2.1518  5.9397
[torch.FloatTensor of size 1x9]

-------------
i = 560000, current_train_loss = [ 0.19760498]
W.data = 

Columns 0 to 7 
 -0.2709  -1.9263  27.1035 -51.9426 -12.6488  34.0350  42.3038  11.0905

Columns 8 to 8 
-47.7049
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0258  1.0776 -6.1706  7.8209  2.9775 -4.0016 -5.8339 -2.0516  5.7008
[torch.FloatTensor of size 1x9]

-------------
i = 640000, current_train_loss = [ 0.1826316]
W.data = 

Columns 0 to 7 
 -0.2604  -2.7759  31.8753 -58.1238 -15.0520  37.0872  46.8037  12.6709

Columns 8 to 8 
-52.1878
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0094  1.0703 -5.8805  7.5397  2.9154 -3.7403 -5.4758 -1.8611  5.5250
[torch.FloatTensor of size 1x9]

-------------
i = 720000, current_train_loss = [ 0.16895524]
W.data = 

Columns 0 to 7 
 -0.2499  -3.5948  36.4563 -64.0504 -17.3437  40.0262  51.0762  14.1600

Columns 8 to 8 
-56.4445
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0680  0.9481 -5.6104  7.2135  2.7379 -3.6547 -5.3283 -1.8932  5.1311
[torch.FloatTensor of size 1x9]

--- 392.82139825820923 seconds ---
--- 6.547023304303488 minutes ---
--- 0.1091170550717248 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
