/home/slurm/slurmd/job9699792/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  60
SLURM_JOBID =  9699792

--Degree_mdl=8 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=60 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.4252848]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841  6.1477  5.4685  4.7929
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841 -6.1477 -5.4685 -4.7929
[torch.FloatTensor of size 1x9]

-------------
i = 80000, current_train_loss = [ 0.32340485]
W.data = 

Columns 0 to 7 
 -0.3121   3.3376  -4.7478  -8.4310   3.4945  11.5015   9.9194   0.1324

Columns 8 to 8 
-14.8333
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-03 *
  0.0064 -0.0461 -0.3432  1.0534  0.1608 -0.7868 -0.9077 -0.2022  1.0703
[torch.FloatTensor of size 1x9]

-------------
i = 160000, current_train_loss = [ 0.29661226]
W.data = 

Columns 0 to 7 
 -0.3278   2.8695  -0.0601 -16.5735   1.1433  16.4389  16.3718   2.0067

Columns 8 to 8 
-21.8171
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0171  1.1055 -7.0677  9.8669  3.5632 -5.2512 -7.4730 -2.4885  7.6309
[torch.FloatTensor of size 1x9]

-------------
i = 240000, current_train_loss = [ 0.27302104]
W.data = 

Columns 0 to 7 
 -0.3184   1.8869   5.7189 -24.3073  -1.7996  20.3756  22.1168   3.9789

Columns 8 to 8 
-27.6035
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0159  1.2792 -7.2418  9.4420  3.7004 -4.6897 -6.9471 -2.4204  6.9582
[torch.FloatTensor of size 1x9]

-------------
i = 320000, current_train_loss = [ 0.2515223]
W.data = 

Columns 0 to 7 
 -0.3061   0.8749  11.4165 -31.6875  -4.7000  24.0294  27.5321   5.8823

Columns 8 to 8 
-32.9957
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
  0.0057  1.2648 -6.9814  9.0344  3.5727 -4.4242 -6.5688 -2.2516  6.6769
[torch.FloatTensor of size 1x9]

-------------
i = 400000, current_train_loss = [ 0.23186228]
W.data = 

Columns 0 to 7 
 -0.2941  -0.0956  16.8699 -38.7370  -7.4889  27.5153  32.7237   7.7081

Columns 8 to 8 
-38.1579
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0473  1.1675 -6.7860  8.5689  3.3529 -4.2825 -6.3278 -2.2156  6.2805
[torch.FloatTensor of size 1x9]

-------------
i = 480000, current_train_loss = [ 0.21395701]
W.data = 

Columns 0 to 7 
 -0.2822  -1.0308  22.0973 -45.4796 -10.1403  30.8611  37.6194   9.4444

Columns 8 to 8 
-43.0479
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0195  1.1424 -6.4010  8.2628  3.2206 -4.1050 -6.0694 -2.1526  5.9390
[torch.FloatTensor of size 1x9]

-------------
i = 560000, current_train_loss = [ 0.19760521]
W.data = 

Columns 0 to 7 
 -0.2709  -1.9263  27.1034 -51.9425 -12.6488  34.0351  42.3038  11.0904

Columns 8 to 8 
-47.7048
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0131  1.0854 -6.1644  7.8266  2.9829 -3.9963 -5.8287 -2.0463  5.7061
[torch.FloatTensor of size 1x9]

-------------
i = 640000, current_train_loss = [ 0.18263139]
W.data = 

Columns 0 to 7 
 -0.2604  -2.7758  31.8752 -58.1237 -15.0521  37.0872  46.8038  12.6708

Columns 8 to 8 
-52.1879
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
 -0.0229  1.0550 -5.8967  7.5230  2.8981 -3.7582 -5.4942 -1.8800  5.5056
[torch.FloatTensor of size 1x9]

-------------
i = 720000, current_train_loss = [ 0.16895546]
W.data = 

Columns 0 to 7 
 -0.2499  -3.5948  36.4561 -64.0501 -17.3438  40.0262  51.0763  14.1598

Columns 8 to 8 
-56.4445
[torch.FloatTensor of size 1x9]

W.grad.data = 
1.00000e-04 *
  0.0175  1.0331 -5.5284  7.2927  2.8147 -3.5799 -5.2551 -1.8214  5.2018
[torch.FloatTensor of size 1x9]

--- 423.5522174835205 seconds ---
--- 7.059203624725342 minutes ---
--- 0.11765339374542236 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
