/home/slurm/slurmd/job9699844/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  49
SLURM_JOBID =  9699844

--Degree_mdl=7 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=49 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.42543319]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841  6.1477  5.4685
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841 -6.1477 -5.4685
[torch.FloatTensor of size 1x8]

-------------
i = 80000, current_train_loss = [ 0.35714465]
W.data = 
 -0.2700   2.8360  -5.4458  -5.2142   6.4380  10.7190   3.5493 -12.5228
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-03 *
  0.0086 -0.1382  0.0745  0.7871 -0.3201 -0.9669 -0.4650  1.0159
[torch.FloatTensor of size 1x8]

-------------
i = 160000, current_train_loss = [ 0.3389152]
W.data = 
 -0.3033   3.1418  -3.8770 -11.7177   7.5046  17.1711   7.1859 -19.0293
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0131  0.2529 -3.6909  8.2531 -0.1509 -7.0345 -4.4933  6.7719
[torch.FloatTensor of size 1x8]

-------------
i = 240000, current_train_loss = [ 0.32382482]
W.data = 
 -0.3071   2.7279  -0.3855 -18.3185   7.2263  22.4082  10.7122 -23.9924
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0143  0.6797 -4.7583  8.2470  0.6811 -6.1886 -4.3304  5.8205
[torch.FloatTensor of size 1x8]

-------------
i = 320000, current_train_loss = [ 0.30958435]
W.data = 
 -0.3031   2.1333   3.5492 -24.8398   6.6169  27.2240  14.1617 -28.4736
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0007  0.7749 -4.9532  8.1021  0.8702 -5.8854 -4.2407  5.4397
[torch.FloatTensor of size 1x8]

-------------
i = 400000, current_train_loss = [ 0.29600617]
W.data = 
 -0.2972   1.5029   7.5289 -31.2268   5.9254  31.8659  17.5192 -32.7516
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0037  0.7787 -4.9252  7.9121  0.8679 -5.7614 -4.2173  5.1557
[torch.FloatTensor of size 1x8]

-------------
i = 480000, current_train_loss = [ 0.28301781]
W.data = 
 -0.2911   0.8747  11.4567 -37.4774   5.2055  36.4121  20.7995 -36.9151
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0103  0.7917 -4.8494  7.7516  0.9160 -5.5300 -4.0087  5.1421
[torch.FloatTensor of size 1x8]

-------------
i = 560000, current_train_loss = [ 0.27073812]
W.data = 
 -0.2844   0.2450  15.3280 -43.5768   4.5467  40.7084  23.9976 -40.9008
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0089  0.7894 -4.7740  7.4559  0.7700 -5.4717 -3.9023  5.1225
[torch.FloatTensor of size 1x8]

-------------
i = 640000, current_train_loss = [ 0.25897878]
W.data = 
 -0.2783  -0.3511  19.0495 -49.5181   3.9115  44.9809  27.1356 -44.8681
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0156  0.7225 -4.5699  7.4240  0.8404 -5.3459 -3.9152  4.7926
[torch.FloatTensor of size 1x8]

-------------
i = 720000, current_train_loss = [ 0.24768929]
W.data = 
 -0.2727  -0.9364  22.7073 -55.3263   3.2055  49.2412  30.1944 -48.7528
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0234  0.7241 -4.5406  7.2276  0.8434 -5.1633 -3.7445  4.7651
[torch.FloatTensor of size 1x8]

--- 644.2236475944519 seconds ---
--- 10.737060793240865 minutes ---
--- 0.17895101322068108 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
