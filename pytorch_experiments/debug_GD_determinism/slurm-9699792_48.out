/home/slurm/slurmd/job9699843/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  48
SLURM_JOBID =  9699843

--Degree_mdl=7 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=48 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.42543319]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841  6.1477  5.4685
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841 -6.1477 -5.4685
[torch.FloatTensor of size 1x8]

-------------
i = 80000, current_train_loss = [ 0.35714468]
W.data = 
 -0.2700   2.8360  -5.4458  -5.2142   6.4380  10.7190   3.5493 -12.5228
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-03 *
  0.0078 -0.1389  0.0738  0.7864 -0.3207 -0.9675 -0.4656  1.0153
[torch.FloatTensor of size 1x8]

-------------
i = 160000, current_train_loss = [ 0.33891544]
W.data = 
 -0.3033   3.1418  -3.8770 -11.7176   7.5046  17.1710   7.1859 -19.0293
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0182  0.2582 -3.6858  8.2579 -0.1461 -7.0296 -4.4884  6.7768
[torch.FloatTensor of size 1x8]

-------------
i = 240000, current_train_loss = [ 0.323825]
W.data = 
 -0.3071   2.7279  -0.3854 -18.3185   7.2263  22.4081  10.7123 -23.9924
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0003  0.6663 -4.7701  8.2361  0.6711 -6.1979 -4.3390  5.8123
[torch.FloatTensor of size 1x8]

-------------
i = 320000, current_train_loss = [ 0.30958426]
W.data = 
 -0.3031   2.1333   3.5492 -24.8398   6.6169  27.2240  14.1616 -28.4735
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0006  0.7746 -4.9531  8.1019  0.8700 -5.8856 -4.2409  5.4395
[torch.FloatTensor of size 1x8]

-------------
i = 400000, current_train_loss = [ 0.2960062]
W.data = 
 -0.2972   1.5029   7.5290 -31.2268   5.9256  31.8657  17.5191 -32.7514
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0067  0.7804 -4.9227  7.9144  0.8699 -5.7599 -4.2161  5.1566
[torch.FloatTensor of size 1x8]

-------------
i = 480000, current_train_loss = [ 0.28301808]
W.data = 
 -0.2911   0.8747  11.4568 -37.4776   5.2058  36.4119  20.7995 -36.9151
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0104  0.8064 -4.8364  7.7638  0.9277 -5.5186 -3.9975  5.1532
[torch.FloatTensor of size 1x8]

-------------
i = 560000, current_train_loss = [ 0.27073824]
W.data = 
 -0.2844   0.2450  15.3280 -43.5770   4.5470  40.7083  23.9975 -40.9008
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0091  0.7732 -4.7887  7.4422  0.7570 -5.4841 -3.9143  5.1106
[torch.FloatTensor of size 1x8]

-------------
i = 640000, current_train_loss = [ 0.25897861]
W.data = 
 -0.2783  -0.3511  19.0495 -49.5183   3.9118  44.9808  27.1356 -44.8680
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0046  0.7425 -4.5518  7.4409  0.8563 -5.3308 -3.9008  4.8065
[torch.FloatTensor of size 1x8]

-------------
i = 720000, current_train_loss = [ 0.24768929]
W.data = 
 -0.2727  -0.9364  22.7074 -55.3264   3.2057  49.2410  30.1944 -48.7528
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0043  0.7395 -4.5269  7.2404  0.8554 -5.1516 -3.7331  4.7763
[torch.FloatTensor of size 1x8]

--- 633.4712021350861 seconds ---
--- 10.557853368918101 minutes ---
--- 0.1759642228153017 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
