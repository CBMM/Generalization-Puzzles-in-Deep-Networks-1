/home/slurm/slurmd/job9699846/slurm_script:268: RankWarning: Polyfit may be poorly conditioned
  c_pinv = np.polyfit( X_train.reshape((N_train,)) , Y_train.reshape((N_train,)) , Degree_mdl )[::-1]
/home/brando90/home_simulation_research/overparametrized_experiments/pytorch_experiments
SLURM_ARRAY_TASK_ID =  50
SLURM_JOBID =  9699846

--Degree_mdl=7 
--degrees=[3, 4, 5, 6, 7, 8] 
--SLURM_ARRAY_TASK_ID=50 

reg_lambda =  0
nb_iter =  800000
N_train=7, N_test=200
--->training SP mdl
nb =  800000
reg_lambda =  0
reg_type =  
reg_lambda=0
-------------
i = 0, current_train_loss = [ 0.42543319]
W.data = 
1.00000e-03 *
  0.0000  8.2479  8.2479  7.7896  7.3314  6.7841  6.1477  5.4685
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-02 *
  0.0000 -8.2479 -8.2479 -7.7896 -7.3314 -6.7841 -6.1477 -5.4685
[torch.FloatTensor of size 1x8]

-------------
i = 80000, current_train_loss = [ 0.35714447]
W.data = 
 -0.2700   2.8360  -5.4459  -5.2142   6.4381  10.7191   3.5493 -12.5229
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-03 *
  0.0079 -0.1385  0.0741  0.7867 -0.3205 -0.9672 -0.4654  1.0155
[torch.FloatTensor of size 1x8]

-------------
i = 160000, current_train_loss = [ 0.33891481]
W.data = 
 -0.3033   3.1419  -3.8771 -11.7177   7.5046  17.1713   7.1860 -19.0296
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0078  0.2488 -3.6953  8.2491 -0.1547 -7.0382 -4.4971  6.7680
[torch.FloatTensor of size 1x8]

-------------
i = 240000, current_train_loss = [ 0.3238245]
W.data = 
 -0.3071   2.7279  -0.3858 -18.3182   7.2262  22.4083  10.7128 -23.9929
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
  0.0143  0.6783 -4.7571  8.2492  0.6834 -6.1868 -4.3294  5.8206
[torch.FloatTensor of size 1x8]

-------------
i = 320000, current_train_loss = [ 0.30958399]
W.data = 
 -0.3031   2.1334   3.5491 -24.8394   6.6158  27.2251  14.1617 -28.4738
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0125  0.7648 -4.9635  8.0932  0.8630 -5.8911 -4.2452  5.4360
[torch.FloatTensor of size 1x8]

-------------
i = 400000, current_train_loss = [ 0.29600546]
W.data = 
 -0.2972   1.5030   7.5289 -31.2269   5.9250  31.8665  17.5194 -32.7520
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0051  0.7799 -4.9272  7.9100  0.8662 -5.7627 -4.2182  5.1551
[torch.FloatTensor of size 1x8]

-------------
i = 480000, current_train_loss = [ 0.28301609]
W.data = 
 -0.2911   0.8748  11.4568 -37.4779   5.2050  36.4137  20.7994 -36.9158
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0152  0.7856 -4.8589  7.7423  0.9072 -5.5384 -4.0171  5.1336
[torch.FloatTensor of size 1x8]

-------------
i = 560000, current_train_loss = [ 0.27073511]
W.data = 
 -0.2844   0.2452  15.3279 -43.5773   4.5451  40.7123  23.9972 -40.9022
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0176  0.7664 -4.7941  7.4403  0.7571 -5.4836 -3.9142  5.1099
[torch.FloatTensor of size 1x8]

-------------
i = 640000, current_train_loss = [ 0.2589744]
W.data = 
 -0.2783  -0.3513  19.0504 -49.5193   3.9093  44.9848  27.1358 -44.8695
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0175  0.7189 -4.5742  7.4200  0.8365 -5.3500 -3.9198  4.7874
[torch.FloatTensor of size 1x8]

-------------
i = 720000, current_train_loss = [ 0.24768598]
W.data = 
 -0.2727  -0.9365  22.7081 -55.3271   3.2031  49.2452  30.1940 -48.7539
[torch.FloatTensor of size 1x8]

W.grad.data = 
1.00000e-04 *
 -0.0045  0.7312 -4.5354  7.2322  0.8478 -5.1591 -3.7406  4.7686
[torch.FloatTensor of size 1x8]

--- 400.99577736854553 seconds ---
--- 6.683262956142426 minutes ---
--- 0.1113877159357071 hours ---

plotting={'save_bulk_experiment': True, 'plotting': False}
lb_test=
